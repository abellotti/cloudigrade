"""
Celery tasks for use in the api v2 app.

Note for developers:
If you find yourself adding a new Celery task, please be aware of how Celery
determines which queue to read and write to work on that task. By default,
Celery tasks will go to a queue named "celery". If you wish to separate a task
onto a different queue (which may make it easier to see the volume of specific
waiting tasks), please be sure to update all the relevant configurations to
use that custom queue. This includes CELERY_TASK_ROUTES in config and the
Celery worker's --queues argument (see deployment-configs.yaml in shiftigrade).
"""
import json
import logging
from datetime import timedelta

from celery import shared_task
from django.conf import settings
from django.contrib.auth.models import User
from django.db import transaction
from django.db.models import Q
from django.utils.translation import gettext as _
from requests.exceptions import BaseHTTPError, RequestException

from api.clouds.aws.models import AwsCloudAccount
from api.clouds.aws.tasks import (
    CLOUD_KEY,
    CLOUD_TYPE_AWS,
    configure_customer_aws_and_create_cloud_account,
    persist_aws_inspection_cluster_results,
    scale_down_cluster,
)
from api.clouds.aws.util import start_image_inspection
from api.models import (
    CloudAccount,
    Instance,
    InstanceEvent,
    MachineImage,
    Run,
)
from api.util import (
    calculate_max_concurrent_usage_from_runs,
    normalize_runs,
    recalculate_runs,
)
from util import aws, insights
from util.celery import retriable_shared_task
from util.misc import get_now

logger = logging.getLogger(__name__)


@retriable_shared_task(autoretry_for=(RequestException, BaseHTTPError))
def create_from_sources_kafka_message(message):
    """
    Create our model objects from the Sources Kafka message.

    Because the Sources API may not always be available, this task must
    gracefully retry if communication with Sources fails unexpectedly.

    If this function succeeds, it spawns another async task to set up the
    customer's AWS account (configure_customer_aws_and_create_cloud_account).

    Args:
        message (dict): the "value" attribute of a message from a Kafka
            topic generated by the Sources service and having event type
            "Authentication.create"
    """
    incomplete_data = False
    account_number = message.get("tenant")
    # We rename "tenant" to "account_number" because the latter appears to be
    # the more accepted term for this value across the platform. The sources
    # service appears to be an anomaly in calling it "tenant".
    if not account_number:
        incomplete_data = True
        logger.error(
            _("Missing expected tenant (aka account number) from message %s"), message,
        )

    username = message.get("username")
    if not username:
        incomplete_data = True
        logger.error(_("Missing expected username from message %s"), message)

    authentication_id = message.get("id")
    if not authentication_id:
        incomplete_data = True
        logger.error(_("Missing expected id from message %s"), message)

    if incomplete_data:
        logger.error(_("Aborting creation. Incorrect message details."))
        return

    authentication = insights.get_sources_authentication(
        account_number, authentication_id
    )
    password = authentication.get("password")
    if not password:
        logger.error(
            _("Missing expected password from authentication for id %s"),
            authentication_id,
        )
        return

    with transaction.atomic():
        user, created = User.objects.get_or_create(username=account_number)
        if created:
            user.set_unusable_password()
            logger.info(
                _("User %s was not found and has been created."), account_number,
            )

    configure_customer_aws_and_create_cloud_account.delay(user.id, username, password)


@retriable_shared_task(autoretry_for=(RuntimeError,))
@aws.rewrap_aws_errors
def delete_from_sources_kafka_message(message):
    """
    Delete our cloud account as per the Sources Kafka message.

    This function is decorated to retry if an unhandled `RuntimeError` is
    raised, which is the exception we raise in `rewrap_aws_errors` if we
    encounter an unexpected error from AWS. This means it should keep retrying
    if AWS is misbehaving.

    Args:
        message (dict): a message from the Kafka topic generated by the
            Sources service and having event type "Authentication.destroy"

    Todo:
        Split out the AWS-specific handling in this function, make it conditional based
        on cloud account type, and have it call another async task function.
    """
    incomplete_data = False
    account_number = message.get("tenant")
    # We rename "tenant" to "account_number" because the latter appears to be
    # the more accepted term for this value across the platform. The sources
    # service appears to be an anomaly in calling it "tenant".
    if not account_number:
        incomplete_data = True
        logger.error(
            _("Missing expected tenant (aka account number) from message %s"), message,
        )

    username = message.get("username")
    if not username:
        incomplete_data = True
        logger.error(_("Missing expected username from message %s"), message)

    if incomplete_data:
        logger.error(_("Aborting deletion. Incorrect message details."))
        return

    try:
        aws_clount = AwsCloudAccount.objects.get(aws_access_key_id=username)
    except AwsCloudAccount.DoesNotExist:
        logger.info(
            _(
                "We do not seem to have a cloud account for %s, "
                "so there is nothing to delete."
            ),
            username,
        )
        return

    try:
        clount = aws_clount.cloud_account.get()
    except CloudAccount.DoesNotExist:
        logger.warning(
            _(
                "We do not seem to have a CloudAccount parent for "
                "AwsCloudAccount object %s, this should not be possible, "
                "deleting."
            ),
            aws_clount,
        )
        aws_clount.delete()
        return

    # Sources does not have any sort of credential validation, or uniqueness.
    # As such, we need to make sure we don't exactly delete someone else's
    # aws account if another user typos their access key id, and then
    # deletes it.
    if clount.user.username != account_number:
        logger.info(
            _(
                "AWS Cloud Account with access_key_id %s does not belong"
                "to a user account number %s, refusing to delete."
            ),
            username,
            account_number,
        )
        return

    clount.delete()


@shared_task
@transaction.atomic
def process_instance_event(event):
    """
    Process instance events that have been saved during log analysis.

    Note:
        When processing power_on type events, this triggers a recalculation of
        ConcurrentUsage objects. If the event is at some point in the
        not-too-recent past, this may take a while as every day since the event
        will get recalculated and saved. We do not anticipate this being a real
        problem in practice, but this has the potential to slow down unit test
        execution over time since their occurred_at values are often static and
        will recede father into the past from "today", resulting in more days
        needing to recalculate. This effect could be mitigated in tests by
        patching parts of the datetime module that are used to find "today".
    """
    after_run = Q(start_time__gt=event.occurred_at)
    during_run = Q(start_time__lte=event.occurred_at, end_time__gt=event.occurred_at)
    during_run_no_end = Q(start_time__lte=event.occurred_at, end_time=None)

    filters = after_run | during_run | during_run_no_end
    instance = Instance.objects.get(id=event.instance_id)

    if Run.objects.filter(filters, instance=instance).exists():
        recalculate_runs(event)
    elif event.event_type == InstanceEvent.TYPE.power_on:
        normalized_runs = normalize_runs([event])
        runs = []
        for index, normalized_run in enumerate(normalized_runs):
            logger.info(
                "Processing run {} of {}".format(index + 1, len(normalized_runs))
            )
            run = Run(
                start_time=normalized_run.start_time,
                end_time=normalized_run.end_time,
                machineimage_id=normalized_run.image_id,
                instance_id=normalized_run.instance_id,
                instance_type=normalized_run.instance_type,
                memory=normalized_run.instance_memory,
                vcpu=normalized_run.instance_vcpu,
            )
            run.save()
            runs.append(run)
        calculate_max_concurrent_usage_from_runs(runs)


@shared_task
@aws.rewrap_aws_errors
def persist_inspection_cluster_results_task():
    """
    Task to run periodically and read houndigrade messages.

    Returns:
        None: Run as an asynchronous Celery task.

    """
    queue_url = aws.get_sqs_queue_url(settings.HOUNDIGRADE_RESULTS_QUEUE_NAME)
    successes, failures = [], []
    for message in aws.yield_messages_from_queue(
        queue_url, settings.AWS_SQS_MAX_HOUNDI_YIELD_COUNT
    ):
        logger.info(_('Processing inspection results with id "%s"'), message.message_id)

        inspection_results = json.loads(message.body)
        if inspection_results.get(CLOUD_KEY) == CLOUD_TYPE_AWS:
            try:
                persist_aws_inspection_cluster_results(inspection_results)
            except Exception as e:
                logger.exception(_("Unexpected error in result processing: %s"), e)
                logger.debug(_("Failed message body is: %s"), message.body)
                failures.append(message)
                continue

            logger.info(
                _("Successfully processed message id %s; deleting from queue."),
                message.message_id,
            )
            aws.delete_messages_from_queue(queue_url, [message])
            successes.append(message)
        else:
            logger.error(
                _('Unsupported cloud type: "%s"'), inspection_results.get(CLOUD_KEY)
            )
            failures.append(message)

    if successes or failures:
        scale_down_cluster.delay()

    return successes, failures


@shared_task
@transaction.atomic
def inspect_pending_images():
    """
    (Re)start inspection of images in PENDING, PREPARING, or INSPECTING status.

    This generally should not be necessary for most images, but if an image
    inspection fails to proceed normally, this function will attempt to run it
    through inspection again.

    This function runs atomically in a transaction to protect against the risk
    of it being called multiple times simultaneously which could result in the
    same image being found and getting multiple inspection tasks.
    """
    updated_since = get_now() - timedelta(
        seconds=settings.INSPECT_PENDING_IMAGES_MIN_AGE
    )
    restartable_statuses = [
        MachineImage.PENDING,
        MachineImage.PREPARING,
        MachineImage.INSPECTING,
    ]
    images = MachineImage.objects.filter(
        status__in=restartable_statuses,
        instance__aws_instance__region__isnull=False,
        updated_at__lt=updated_since,
    ).distinct()
    logger.info(
        _(
            "Found %(number)s images for inspection that have not updated "
            "since %(updated_time)s"
        ),
        {"number": images.count(), "updated_time": updated_since},
    )

    for image in images:
        instance = image.instance_set.filter(aws_instance__region__isnull=False).first()
        arn = instance.cloud_account.content_object.account_arn
        ami_id = image.content_object.ec2_ami_id
        region = instance.content_object.region
        start_image_inspection(arn, ami_id, region)
