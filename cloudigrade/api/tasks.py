"""
Celery tasks for use in the api v2 app.

Note for developers:
If you find yourself adding a new Celery task, please be aware of how Celery
determines which queue to read and write to work on that task. By default,
Celery tasks will go to a queue named "celery". If you wish to separate a task
onto a different queue (which may make it easier to see the volume of specific
waiting tasks), please be sure to update all the relevant configurations to
use that custom queue. This includes CELERY_TASK_ROUTES in config and the
Celery worker's --queues argument (see deployment-configs.yaml in shiftigrade).
"""
import json
import logging
from datetime import timedelta

from celery import shared_task
from django.conf import settings
from django.contrib.auth.models import User
from django.db import transaction
from django.db.models import Q
from django.utils.translation import gettext as _
from requests.exceptions import BaseHTTPError, RequestException

from api.clouds.aws.tasks import (
    CLOUD_KEY,
    CLOUD_TYPE_AWS,
    configure_customer_aws_and_create_cloud_account,
    persist_aws_inspection_cluster_results,
    scale_down_cluster,
)
from api.clouds.aws.util import start_image_inspection, update_aws_cloud_account
from api.models import (
    CloudAccount,
    Instance,
    InstanceEvent,
    MachineImage,
    Run,
)
from api.util import (
    calculate_max_concurrent_usage_from_runs,
    normalize_runs,
    recalculate_runs,
)
from util import aws, insights
from util.celery import retriable_shared_task
from util.misc import get_now

logger = logging.getLogger(__name__)


@retriable_shared_task(autoretry_for=(RequestException, BaseHTTPError))
def create_from_sources_kafka_message(message, headers):
    """
    Create our model objects from the Sources Kafka message.

    Because the Sources API may not always be available, this task must
    gracefully retry if communication with Sources fails unexpectedly.

    If this function succeeds, it spawns another async task to set up the
    customer's AWS account (configure_customer_aws_and_create_cloud_account).

    Args:
        message (dict): the "value" attribute of a message from a Kafka
            topic generated by the Sources service and having event type
            "Authentication.create"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "Authentication.create"

    """
    incomplete_data = False

    auth_header = insights.get_x_rh_identity_header(headers)
    if not auth_header:
        incomplete_data = True
        logger.error(
            _("Missing expected auth header from message %s, headers %s"),
            message,
            headers,
        )

    account_number = auth_header.get("identity", {}).get("account_number")
    if not account_number:
        incomplete_data = True
        logger.error(
            _("Missing expected account number from message %s, headers %s"),
            message,
            headers,
        )

    authentication_id = message.get("id")
    if not authentication_id:
        incomplete_data = True
        logger.error(
            _("Missing expected id from message %s, headers %s"), message, headers
        )

    if incomplete_data:
        logger.error(_("Aborting creation. Incorrect message details."))
        return

    authentication = insights.get_sources_authentication(
        account_number, authentication_id
    )
    if not authentication:
        logger.info(
            _(
                "Authentication ID %(authentication_id)s for account number "
                "%(account_number)s does not exist; aborting cloud account creation."
            ),
            {"authentication_id": authentication_id, "account_number": account_number},
        )
        return
    endpoint_id = authentication.get("resource_id")
    endpoint = insights.get_sources_endpoint(account_number, endpoint_id)
    if not endpoint:
        logger.info(
            _(
                "Endpoint ID %(endpoint_id)s for account number "
                "%(account_number)s does not exist; aborting cloud account creation."
            ),
            {"endpoint_id": endpoint_id, "account_number": account_number},
        )
        return

    source_id = endpoint.get("source_id")

    password = authentication.get("password")

    if not password:
        logger.error(
            _("Missing expected password from authentication for id %s"),
            authentication_id,
        )
        return

    with transaction.atomic():
        user, created = User.objects.get_or_create(username=account_number)
        if created:
            user.set_unusable_password()
            logger.info(
                _("User %s was not found and has been created."), account_number,
            )

    # Conditionalize the logic for different cloud providers
    if message.get("authtype") == settings.SOURCES_CLOUDMETER_ARN_AUTHTYPE:
        configure_customer_aws_and_create_cloud_account.delay(
            user.id, password, authentication_id, endpoint_id, source_id
        )


@retriable_shared_task(autoretry_for=(RuntimeError,))  # noqa: C901
@aws.rewrap_aws_errors
def delete_from_sources_kafka_message(message, headers, event_type):
    """
    Delete our cloud account as per the Sources Kafka message.

    This function is decorated to retry if an unhandled `RuntimeError` is
    raised, which is the exception we raise in `rewrap_aws_errors` if we
    encounter an unexpected error from AWS. This means it should keep retrying
    if AWS is misbehaving.

    Args:
        message (dict): a message from the Kafka topic generated by the
            Sources service and having event type "Authentication.destroy"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "Authentication.destroy", "Endpoint.destroy" or "Source.destroy"
        event_type (str): A string describing the type of destroy event.

    """
    incomplete_data = False

    auth_header = insights.get_x_rh_identity_header(headers)
    if not auth_header:
        incomplete_data = True
        logger.error(
            _("Missing expected auth header from message %s, headers %s"),
            message,
            headers,
        )

    account_number = auth_header.get("identity", {}).get("account_number")
    if not account_number:
        incomplete_data = True
        logger.error(
            _("Missing expected account number from message %s, headers %s"),
            message,
            headers,
        )

    platform_id = message.get("id")
    if not platform_id:
        incomplete_data = True
        logger.error(
            _("Missing expected id from message %s, headers %s"), message, headers
        )

    if incomplete_data:
        logger.error(_("Aborting deletion. Incorrect message details."))
        return
    try:
        clount = None
        if event_type == settings.SOURCE_DESTROY_EVENT:
            clount = CloudAccount.objects.filter(platform_source_id=platform_id)
        elif event_type == settings.ENDPOINT_DESTROY_EVENT:
            clount = CloudAccount.objects.filter(platform_endpoint_id=platform_id)
        elif event_type == settings.AUTHENTICATION_DESTROY_EVENT:
            clount = CloudAccount.objects.filter(platform_authentication_id=platform_id)

        if clount:
            clount.delete()

    # We could receive and queue up several delete tasks for the same clount,
    # handle the error in case that happens.
    except CloudAccount.DoesNotExist:
        logger.info(
            _(
                "The cloud account associated with delete message %s has "
                "already been deleted, so there is nothing else to delete."
            ),
            message,
        )
        return


@retriable_shared_task(autoretry_for=(RequestException, BaseHTTPError, RuntimeError))
@aws.rewrap_aws_errors
def update_from_source_kafka_message(message, headers):
    """
    Update our model objects from the Sources Kafka message.

    Because the Sources API may not always be available, this task must
    gracefully retry if communication with Sources fails unexpectedly.

    This function is also decorated to retry if an unhandled `RuntimeError` is
    raised, which is the exception we raise in `rewrap_aws_errors` if we
    encounter an unexpected error from AWS. This means it should keep retrying
    if AWS is misbehaving.

    Args:
        message (dict): the "value" attribute of a message from a Kafka
            topic generated by the Sources service and having event type
            "Authentication.update"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "Authentication.update"

    """
    incomplete_data = False
    auth_header = insights.get_x_rh_identity_header(headers)

    if not auth_header:
        incomplete_data = True
        logger.error(
            _("Missing expected auth header from message %s, headers %s"),
            message,
            headers,
        )

    account_number = auth_header.get("identity", {}).get("account_number")
    if not account_number:
        incomplete_data = True
        logger.error(
            _("Missing expected account number from message %s, headers %s"),
            message,
            headers,
        )

    authentication_id = message.get("id")
    if not authentication_id:
        incomplete_data = True
        logger.error(
            _("Missing expected id from message %s, headers %s"), message, headers
        )

    if incomplete_data:
        logger.error(_("Aborting update. Incorrect message details."))
        return

    try:
        clount = CloudAccount.objects.get(platform_authentication_id=authentication_id)

        authentication = insights.get_sources_authentication(
            account_number, authentication_id
        )
        endpoint_id = authentication.get("resource_id")
        endpoint = insights.get_sources_endpoint(account_number, endpoint_id)
        source_id = endpoint.get("source_id")

        # If the Authentication being updated is arn, do arn things.
        # The kafka messgae does not always include authtype, so we get this from
        # the sources API call
        if authentication.get("authtype") == settings.SOURCES_CLOUDMETER_ARN_AUTHTYPE:
            update_aws_cloud_account(
                clount,
                authentication.get("password"),
                account_number,
                authentication_id,
                endpoint_id,
                source_id,
            )
    except CloudAccount.DoesNotExist:
        logger.debug(
            _(
                "The updated authentication with ID %s and account number %s "
                "is not managed by cloud meter."
            ),
            authentication_id,
            account_number,
        )
        return


@shared_task
@transaction.atomic
def process_instance_event(event):
    """
    Process instance events that have been saved during log analysis.

    Note:
        When processing power_on type events, this triggers a recalculation of
        ConcurrentUsage objects. If the event is at some point in the
        not-too-recent past, this may take a while as every day since the event
        will get recalculated and saved. We do not anticipate this being a real
        problem in practice, but this has the potential to slow down unit test
        execution over time since their occurred_at values are often static and
        will recede father into the past from "today", resulting in more days
        needing to recalculate. This effect could be mitigated in tests by
        patching parts of the datetime module that are used to find "today".
    """
    after_run = Q(start_time__gt=event.occurred_at)
    during_run = Q(start_time__lte=event.occurred_at, end_time__gt=event.occurred_at)
    during_run_no_end = Q(start_time__lte=event.occurred_at, end_time=None)

    filters = after_run | during_run | during_run_no_end
    instance = Instance.objects.get(id=event.instance_id)

    if Run.objects.filter(filters, instance=instance).exists():
        recalculate_runs(event)
    elif event.event_type == InstanceEvent.TYPE.power_on:
        normalized_runs = normalize_runs([event])
        runs = []
        for index, normalized_run in enumerate(normalized_runs):
            logger.info(
                "Processing run {} of {}".format(index + 1, len(normalized_runs))
            )
            run = Run(
                start_time=normalized_run.start_time,
                end_time=normalized_run.end_time,
                machineimage_id=normalized_run.image_id,
                instance_id=normalized_run.instance_id,
                instance_type=normalized_run.instance_type,
                memory=normalized_run.instance_memory,
                vcpu=normalized_run.instance_vcpu,
            )
            run.save()
            runs.append(run)
        calculate_max_concurrent_usage_from_runs(runs)


@shared_task
@aws.rewrap_aws_errors
def persist_inspection_cluster_results_task():
    """
    Task to run periodically and read houndigrade messages.

    Returns:
        None: Run as an asynchronous Celery task.

    """
    queue_url = aws.get_sqs_queue_url(settings.HOUNDIGRADE_RESULTS_QUEUE_NAME)
    successes, failures = [], []
    for message in aws.yield_messages_from_queue(
        queue_url, settings.AWS_SQS_MAX_HOUNDI_YIELD_COUNT
    ):
        logger.info(_('Processing inspection results with id "%s"'), message.message_id)

        inspection_results = json.loads(message.body)
        if inspection_results.get(CLOUD_KEY) == CLOUD_TYPE_AWS:
            try:
                persist_aws_inspection_cluster_results(inspection_results)
            except Exception as e:
                logger.exception(_("Unexpected error in result processing: %s"), e)
                logger.debug(_("Failed message body is: %s"), message.body)
                failures.append(message)
                continue

            logger.info(
                _("Successfully processed message id %s; deleting from queue."),
                message.message_id,
            )
            aws.delete_messages_from_queue(queue_url, [message])
            successes.append(message)
        else:
            logger.error(
                _('Unsupported cloud type: "%s"'), inspection_results.get(CLOUD_KEY)
            )
            failures.append(message)

    if successes or failures:
        scale_down_cluster.delay()

    return successes, failures


@shared_task
@transaction.atomic
def inspect_pending_images():
    """
    (Re)start inspection of images in PENDING, PREPARING, or INSPECTING status.

    This generally should not be necessary for most images, but if an image
    inspection fails to proceed normally, this function will attempt to run it
    through inspection again.

    This function runs atomically in a transaction to protect against the risk
    of it being called multiple times simultaneously which could result in the
    same image being found and getting multiple inspection tasks.
    """
    updated_since = get_now() - timedelta(
        seconds=settings.INSPECT_PENDING_IMAGES_MIN_AGE
    )
    restartable_statuses = [
        MachineImage.PENDING,
        MachineImage.PREPARING,
        MachineImage.INSPECTING,
    ]
    images = MachineImage.objects.filter(
        status__in=restartable_statuses,
        instance__aws_instance__region__isnull=False,
        updated_at__lt=updated_since,
    ).distinct()
    logger.info(
        _(
            "Found %(number)s images for inspection that have not updated "
            "since %(updated_time)s"
        ),
        {"number": images.count(), "updated_time": updated_since},
    )

    for image in images:
        instance = image.instance_set.filter(aws_instance__region__isnull=False).first()
        arn = instance.cloud_account.content_object.account_arn
        ami_id = image.content_object.ec2_ami_id
        region = instance.content_object.region
        start_image_inspection(arn, ami_id, region)
